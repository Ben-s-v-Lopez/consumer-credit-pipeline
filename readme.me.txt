# Consumer Credit Data Pipeline

## Overview
This project implements a robust data pipeline for processing consumer credit lending data, demonstrating modern data engineering practices including data validation, transformation, and quality control. The pipeline is built using Python, PySpark, and follows industry best practices for data processing and analysis.

## Features
- Data validation and quality checks
- Credit risk scoring
- Loan categorization
- Comprehensive logging and monitoring
- Configurable processing rules
- Summary report generation
- Support for multiple input/output formats

## Project Structure
```
consumer-credit-pipeline/
├── src/
│   ├── pipeline.py          # Main pipeline implementation
│   └── data_generator.py    # Sample data generator
├── data/
│   ├── raw/                 # Raw input data
│   └── processed/           # Processed output data
├── config.yml               # Pipeline configuration
├── requirements.txt         # Project dependencies
├── README.md               # This file
└── pipeline.log            # Processing logs
```

## Requirements
- Python 3.8+
- Apache Spark 3.x
- Dependencies listed in requirements.txt

## Installation
1. Clone the repository:
```bash
git clone https://github.com/yourusername/consumer-credit-pipeline.git
cd consumer-credit-pipeline
```

2. Create and activate a virtual environment:
```bash
python -m venv venv
source venv/bin/activate  # On Windows: venv\Scripts\activate
```

3. Install dependencies:
```bash
pip install -r requirements.txt
```

## Usage

### Generate Sample Data
```bash
python src/data_generator.py
```
This will create sample credit lending data in `data/raw/credit_data.csv`.

### Run the Pipeline
```bash
python src/pipeline.py
```

The pipeline will:
1. Load data from the configured input path
2. Perform data validation
3. Apply transformations and risk scoring
4. Generate a summary report
5. Save processed data to the configured output path

### Configuration
The pipeline behavior can be customized through `config.yml`:
- Required fields for validation
- Numerical fields for outlier detection
- Risk scoring weights
- Spark configuration

## Sample Data Format
The pipeline expects input data with the following schema:
```json
{
  "loan_id": "LOAN00000001",
  "customer_id": "CUST000001",
  "loan_amount": 8500.25,
  "interest_rate": 5.2,
  "start_date": "2023-01-15",
  "end_date": "2024-01-15",
  "payment_frequency": "MONTHLY",
  "loan_purpose": "DEBT_CONSOLIDATION",
  "credit_score": 720,
  "debt_to_income_ratio": 0.35,
  "payment_history_score": 85
}
```

## Output
The pipeline generates:
1. Processed data with additional fields:
   - risk_score
   - risk_category
   - loan_duration_days
2. Summary report (summary_report.yml) with:
   - Total loans processed
   - Risk distribution
   - Average loan duration
   - Total loan value

## Logging
The pipeline logs all operations to both console and `pipeline.log`, including:
- Data validation results
- Processing statistics
- Errors and warnings

## Contributing
1. Fork the repository
2. Create your feature branch (`git checkout -b feature/AmazingFeature`)
3. Commit your changes (`git commit -m 'Add some AmazingFeature'`)
4. Push to the branch (`git push origin feature/AmazingFeature`)
5. Open a Pull Request

## License
This project is licensed under the MIT License - see the LICENSE file for details.
